\chapter{Data retrieval and preprocessing}
In this chapter we will present our pipeline intended for data retrieval and preprocessing.
Since we used a considerable amount of third party libraries and tools, we will explain them in detail and we will clarify our reasons to use them.
We will also provide some basic statistics of data retrieved.

\section{Brief pipeline overview}
Our pipeline consists of python scripts and third party bioinformatics software.
When writing the code, we have taken care of its readability, sustainability and extensibility.
The time of execution of bioinformatics programs can be enormous, so we divided our pipeline to various steps.
After each step, there is a checkpoint, which ensures we will not need to run this step again in case of unexpected crash of pipeline.
The pipeline starts with downloading of publicly available data from 3 sources - NCBI, viralzone and phagesdb.
After downloading we merge all records into one file and eliminate duplicated records.
Later on we predict bacteriophages genes with Prokka \cite{prokka}.
Next step was to cluster all predicted genes into clusters based on similarity of protein sequences.
For this purpose we use CrocoBLAST \cite{crocoblast}, EMBOSS needle software \cite{needle} and mcl algorithm \cite{mcl}.
After clustering we created matrix, where rows were individual bacteriophages and columns were our clusters.
In this matrix 1 represented, that given phage contains a gene from given cluster and 0 represented it does not contain a gene from a given cluster.
This matrix was later used in analysis as input to machine learning algorithms.

\section{Snakefile}
Snakefile is the main script, which executes all other components of our program. 
It is written in Snakemake \cite{snakemake}, workflow engine designed for writing reproducible pipelines in bioinformatics. 
Snakemake is inspired by GNU make, but it use python-like syntax with elements similar to pseudo code.
Furthermore, it is fully portable, with dependency only on python.
Snakefile consists of rules, where each rule is defined by its input files, output files and shell commands.
These are commands needed to execute to produce output files from input files.
When the Snakemake is executed, by default, it runs first rule in particular Snakefile. 
If the rule cannot be completed, because of missing input files, it scan through the whole Snakefile and look for a rule, which is capable of creating desired file. 
This process is repeated until there is rule which can be completed or rule whose input is not possible to create by any other rule.
By this approach it is ensured we do not run any unnecessary rules and already completed rules.
This is an important feature for our work as some rules can take several hours to complete even on a powerful server. 

\section{Downloading}
First rule to complete in our workflow was downloading data from publicly available databases.
We made this rule easily extensible for new sources of information.
New database can be added by writing a new download script, 
naming it \verb|{script_dir}\download_from_{db}.py| and appending this name into variable \verb|DATABASES| in config file. We implemented downloading from 3 sources - NCBI \cite{ncbi}, viralzone \cite{viralzone}, phagesdb  \cite{phagesdb}.

\subsection{Downloading from NCBI}
NCBI, National Center for Biotechnology Information, was the largest source of data for our pipeline.
Their nucleotide database should contain all published genomes.
Downloading of these genomes from their database is possible through web interface or through API.
In our script we used python library Biopython \cite{biopython}, which provides python wrapper around NCBI API.
Besides, Biopython offers useful classes for work with standard file formats used in bioinformatics.
Downloaded data were stored in four files with extensions genomes.fasta, genomes.conversion, genes.fasta, genes.conversion.
Fasta files were described in chapter 1.
Conversion files are tab separated files containing metadata about particular downloaded sequences.
In file with extension genomes.conversion these metadata are phage identifier in our database, phage identifier in external database, phage name and its hosts.
In file with extension genes.conversion these metadata are gene identifier in our database, our phage identifier of bacteriophage with this gene, identifier of gene from external database, its position in phage genome and function.
Reasons for creation of our internal identifiers were inconsistency in naming conventions between different databases.
Our software downloaded all records in NCBI nucleotide database satisfying query 
\begin{verbatim}
'phage[Title] AND (complete genome[Title] 
OR complete sequence[Title]) AND (viruses[filter] 
AND biomol_genomic[PROP] AND ("10000"[SLEN] : "100000000"[SLEN]))'
\end{verbatim}. There was 6704 such records. 

\subsection{Downloading from viralzone}
Viralzone is a website dedicated to viruses. 
It provides high quality data about viruses, including bacteriophages.
We used this website to get query similar to query in previous section.
This new query was used by the same program as our query.
There was 2107 records satisfying query from viralzone. (18-03-08)

\subsection{Downloading from PhagesDB}
PhagesDB is an independent database from NCBI. 
This database is specialized for Actinobacteriophages, so it has less phage records, but it is designed to avoid the time between sequencing and data availability.
Authors declare, at the time of their publication, there was more than 600 records of bacteriophages in phagesdb that were not yet in GenBank.

Furthermore, PhagesDB stores more biological relevant data which are easily obtainable through its RESTful Application Programming Interface.
In our work we used this API to retrieve all genomic sequences with their hosts and annotations.
Eventually we ended up with 2491 phage records from PhagesDB 

\section{Merging and removing of duplicated records}
All download scripts produced four types of output - \verb|<db>.genomes.fasta| \verb|<db>.genomes.conversion| \verb|<db>.genes.fasta| \verb|<db>.genes.conversion|. 
We were aware of high redundancy in our datasets, mainly because many of these records was submitted to more databases at once.
To solve this issue, we merged datasets together and removed duplicated records.
For merging we used standard unix command \verb|cat| for all files with the same type.
For removing duplicated we created our python script.
We looked on all fasta records in merged \verb|genomes.fasta| and if there were more than one with the same sequence, we wrote the one with lesser internal identifier into output fasta file.
We also changed other internal identifiers in files \verb|genomes.conversion| and \verb|genes.conversion| into this identifier.
This approach preserves the relationships between one particular sequence and all data related to it.
Therefore we are able to track phages, based on their internal identifier, to their source databases and also connect them with data already downloaded.
After duplicate removing our dataset consisted of 6277 phage records.

\section{Gene identification}
Despite of downloading annotated sequences with genes, we do not use those genes further in out workflow.
We made this decision because the inconsistency of gene annotations between different databases and different records could be potentially high.
To ensure predictions will be consistent along the entire dataset, we predicted all genes and their annotations by one software - Prokka.
In addition, this software uses regularly updated databases, which guarantee the most up-to-date annotations.
After annotation we extracted all genes with our script.
The output of this program were files \verb|annotated.genes.fasta| and \verb|annotated.genes.conversion|.

\section{Splitting to training set and testing set}


\section{Local alignment of genes}

\section{Clustering}
% http://www.paccanarolab.org/scps/
\subsection{Markov Cluster Algorithm}
\subsection{Markov Cluster Algorithm with global alignment}
\subsection{Spectral clustering}

\section{Cluster annotation}

\section{Creation of matrix}
