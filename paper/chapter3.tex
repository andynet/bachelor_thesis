\chapter{Data retrieval and preprocessing}
In this chapter we will present parts of our pipeline intended for data retrieval and preprocessing.
Since we used a considerable amount of third party libraries and tools, we will explain them in detail and we will clarify our reasons to use them.
We will also provide some basic statistics of data retrieved.

\section{Pipeline overview}
Our pipeline consists of python scripts and third party bioinformatics software.
When writing the code, we have taken care of its readability, sustainability and extensibility.

As the main script we used Snakefile written in Snakemake.
The Snakemake is a workflow management system designed for writing reproducible bioinformatics pipelines.
It is inspired by GNU make, but it use python-like syntax with elements similar to pseudo code.
Furthermore, it is fully portable, with dependency only on python.
Snakefile consists of rules, where each rule is defined by its input files, output files and shell commands.
These are commands needed to execute in order to produce output files from input files.
When the Snakemake is executed, by default, it runs first rule in particular Snakefile. 
If the rule cannot be completed, because of missing input files, it scan through the whole Snakefile and look for a rule, which is capable of creating desired file. 
This process is repeated until there is rule which can be completed or rule whose input is not possible to create by any other rule.
In the former case the execution starts running, in the latter case an error message is displayed.
By this approach it is ensured we do not run any unnecessary rules and already completed rules.
This is an important feature for our program as some rules can take several hours to complete even on a powerful server. 
Another pleasant characteristics of the Snakemake engine is the ability to produce graphical visualisation of particular Snakefile in format of directed acyclic graph.
Simplified graphical representation of our pipeline is in the Figure (\ref{fig:dag}).

The pipeline starts with downloading of publicly available data.
After downloading we merge all records and eliminate duplicated records.
Later on, we predict bacteriophages genes.
Consequently, phage genomes with their corresponding genes are split into training and testing set.
Similarity between genes from training set are calculated and based on those, clusters of similar genes are produced. 
From these clusters the binary matrix is created.
This matrix is later used in analysis as input to machine learning algorithms.

\begin{figure}[h]
\includegraphics[height=\textheight]{./images/mcl.png}
\centering
\caption{Workflow visualization}
\label{fig:dag}
\end{figure}

\section{Downloading}
Since the first step in our pipeline is downloading of data from publicly available databases, we will provide more detailed description of these sources.
In our pipeline we implemented downloading from three databases.
Although they cover the most of currently sequenced and published phages, we made this step easily extensible for new sources of information.
New source can be added by writing a new download script, naming it \verb|{script_dir}\download_from_{db}.py| and appending this name into variable \verb|DATABASES| in Snakefile. 

\subsection{Downloading from GenBank}
National Center for Biotechnology Information (NCBI) provides an access to GenBank\cite{genbank} database.
This database is a comprehensive source of genomic data with more than 200 million sequences.
Sequences are primarily submitted by individuals all around the globe.
Moreover, GenBank is daily synchronized with European Nucleotide Archive and DNA Data Bank of Japan, which ensures that data are always up-to-date with human knowledge.
NCBI administer GenBank database free of charge and give researchers the possibility to access data through various interfaces as web-based retrieval services, FTP and Entrez\cite{entrez}.
Despite of these facts, there are also shortcomings of using GenBank.
In the time of Next Generation Sequencing the amount of data flowing into GenBank database every day is enormous.
Therefore it is unreasonable to check all data.
This is the cause of redundancy of sequences and sometimes contradictions between information in system.

In our work, data from GenBank was downloaded through our custom script.
We obtained data using python library Biopython \cite{biopython}, which implements python wrapper NCBI Entrez.
Besides, Biopython was also useful for its classes enabling easy manipulation with standard file formats used in bioinformatics.
When downloading sequences, we also created unique identifiers for each record.
Those were used later in pipeline.
Reasons behind the decision to use custom identifiers were ability to remove duplicated sequences and ability to find out possibly multiple sources of each sequence in our dataset.
Downloading from GenBank was our largest source of data with 6704 downloaded records.

\subsection{Downloading from ViralZone}
ViralZone is a website dedicated to viruses. 
It provides highly reliable data about viruses, including bacteriophages.
Information about structure of capsid and genome, life cycle, replication mechanisms, taxonomy, geographical location and host are included.
This website does not store sequences internally, rather it delivers links to RefSeq\cite{refseq} database.
Compared to GenBank, RefSeq database contains fewer sequences, but all of these sequences are curated and manually reviewed.

Our custom script was used to download records from RefSeq database.
Although, large portion of sequences downloaded was identical with GenBank records, some sequences were unique.
Another advantage in performing this action was that it enabled us to pair records in our dataset with full information from ViralZone portal.
By this process we obtained 2107 records. 

% end of session 2018.04.18
% wc chapter* = 32521 letters

\subsection{Downloading from PhagesDB}
PhagesDB is a database specialized for bacteriophages infecting bacteria from phylum Actinobacteria.
This phylum is of great importance, because of its contribution to soil system.
Phylum also contains genus Mycobacterium, which includes pathogens causing tuberculosis and leprosy in humans.
The database was designed to avoid the time between sequencing and data availability.
Authors declare, at the time of their publication, there was more than 600 records of bacteriophages that were not yet in GenBank.
Furthermore, PhagesDB stores more biologically relevant data which are easily obtainable through its Application Programming Interface (API).
These biological data contains discovery details, sequencing details, characterization details, sequence file and plaque picture.
From this database we downloaded 2491 phage records with our automatized script using API.

\section{Merging and removing of duplicated records}
We were aware of high redundancy in our datasets, mainly because many of those records was submitted to more databases at once.
To solve this issue, we merged datasets together and removed duplicated records.
For merging we used standard unix command \verb|cat|.
For removing duplicated sequences we created python script.
This script made use of custom identifiers, which were assigned to every sequence downloaded.
If more identical sequences were found their identifiers were rewritten with identifier of the first sequence.
This approach preserved the relationships between one particular sequence and all data related to it.
Therefore we were able to track phages, based on their identifiers, to their source databases and also connect them with all data already downloaded.
After removing of duplicated sequences our dataset contained 6277 phage records.

\section{Gene identification}
Next step in the pipeline was to identify genes in the dataset.
Although gene annotations of particular genomes can be found in most databases, we decided to annotate sequences from scratch.
This decision was based on our request for consistency of predicted and annotated genes.
Another advantage of annotating from scratch is that annotations will always be up-to-date with current human knowledge.

\subsection{Prokka}
For the purpose of gene identification we used third party software Prokka\cite{prokka}.
Prokka uses external tools for identification and annotation of genes.
First, coordinates of coding DNA sequences (CDS) are found with Prodigal tool\cite{prodigal}.
CDSs are regions of genome that almost always start with AUG codon and end with stop codon.
These regions can be directly translated into amino acid chains using standard codon table.
After locations of genes were predicted, Prokka starts to annotate functions of all CDSs.
This is usually done through comparing of sequence to database of sequences with experimentally determined function.
The function of protein with the best match is than assigned to new CDS.
Prokka also uses this approach, but it searches through multiple databases.
Starting with the most reliable source, which is usually the smallest, it scans all the databases, always continuing with the less accurate one.
The databases used with their corresponding order as as follows:
An optional user-defined database, UniProt\cite{uniprot}, RefSeq\cite{refseq}, Pfam\cite{pfam} and TIGRFAM\cite{tigrfam}.
If no match is found across databases, protein is labelled as \verb|hypothetical protein|. 

% end of session 2018.04.12
% wc chapter* = 33657 letters

\subsubsection{UniProt}
The UniProt database is a huge collection of protein sequences and their corresponding detailed information.
It contains more than 60 millions of proteins.
Prokka uses just a small fraction of proteins backed up by experimental evidences.
This typically provide information about around 50\% of queried proteins.

\subsubsection{RefSeq}
RefSeq, in addition to genomic sequences, provides also information about protein sequences.
It contains more than 2.5 millions of protein records.
Multiple sources are integrated in annotation of genes.
Furthermore, all records are curated by NCBI staff members.

\subsubsection{Pfam}
Pfam is a database consisting of protein families and domain records.
Protein families are groups of protein sharing evolutionary history.
This shared evolutionary history is often expressed by closely related functions and sequence similarity.
Protein domains are parts of protein sequence responsible for a particular interaction.
Members of the same protein domain usually share high sequence similarity.
Protein families and domains are mostly characterized by Profile Hidden Markov Models.
Pfam incorporate more than 6100 of those models.

\subsubsection{TIGRFAM}
TIGRFAM, similarly as Pfam, contains protein families characterized by Profile Hidden Markov Models.
It contains more than 4200 models with comprehensive description of family structure and function.

\vspace{\baselineskip}

All of the aforementioned databases are regularly updated, which guarantee the most up-to-data annotations. 
After annotation, resulting genes were selected and saved to files in format suitable for further use in the pipeline.

\section{Training set, testing set and other}
At the beginning of this step our dataset consisted of phage genomic records, their corresponding genes, information about phage hosts and functional annotation of genes.
As our work used techniques of supervised machine learning, we needed to split the dataset to training set and testing set.
Furthermore, we created set for other records, which we decided not to use due to missing information about host or due to host outside of our group of interest.
We decided to group phages according to genus of their hosts.
After calculating number of phages in each group, we selected first eight genera with the highest count of records as groups of our interest.
These were Mycobacterium, Streptococcus, Escherichia, Gordonia, Arthrobacter, Pseudomonas, Lactococcus and Staphylococcus. 
Number of phages within each group can be found in the Table (\ref{tab:counts}).
All other genera were excluded from the dataset due to insufficient amount of samples.
Phages without information about their hosts were also excluded.
Subsequently, we divided remaining data into training set and testing set at a ratio 4:1.
Resulting training set included 2787 records of bacteriophages and resulting testing set included 699 records. 

\begin{table}
  \centering
    \begin{tabular}{ l  r  l  r }
      \hline
      genera & count & genera & count \\
      \hline
      Mycobacterium & 1619 & Streptococcus & 354 \\
      Escherichia & 323 & Gordonia & 293 \\
      Arthrobacter & 240 & Pseudomonas & 236 \\
      Lactococcus & 219 & Staphylococcus & 184 \\
      \hline
    \end{tabular}
    \caption{Counts of records within genera}
    \label{tab:counts}
\end{table}

%--------deeply described parts-------------------------------------------------
\input{section3.6.tex}
\input{section3.7.tex}
%--------end of deeply described parts------------------------------------------

\section{Cluster annotation}
Resulting clusters were annotated for their function.
We expected proteins with similar function to be included in same cluster.
For functional annotation of particular proteins we used software InterProScan\cite{interpro}.
This tool scans given protein sequences against the protein signatures in databases PROSITE\cite{prosite}, PRINTS\cite{prints}, Pfam\cite{pfam}, ProDom\cite{prodom} and SMART\cite{smart}.
After acquiring of annotations for all proteins in particular cluster, we calculated number of occurrences of each protein function.
These statistics represents our annotation of certain cluster.
Our expectation of clusters containing proteins with similar function was met in most cases, although the information about particular proteins were sparse with a lot of proteins without any information.
Therefore, reasonable clustering was achieved and is supported by our knowledge of phage biology.

\section{Creation of matrix}
One of the most crucial part of our analysis was binary matrix created in this step.
Rows in this matrix represented particular phage record and columns represented particular protein cluster.
The entry $a_{i,j}$ in matrix was filled with $1$ if phage $i$ contained gene from cluster $j$ and $0$ otherwise.
Custom python script and files produced in previous steps were used for this task.
Resulting matrix contained 2787 rows and 15017 columns and served as a main input file for machine learning algorithms.


